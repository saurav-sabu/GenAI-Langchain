{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXDsr1KdQ-XR"
      },
      "outputs": [],
      "source": [
        "# Install necessary Python packages for working with LangChain, Weaviate, and PDFs.\n",
        "! pip install langchain-openai\n",
        "! pip install langchain-community\n",
        "! pip install weaviate-client\n",
        "! pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyPDFDirectoryLoader to load PDF documents from a directory.\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "# Load PDF documents from the \"data\" directory.\n",
        "loader = PyPDFDirectoryLoader(\"data\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "lIPjaSMr62Nd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the loaded data.\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VhD9ZX89JJF",
        "outputId": "e3cab9df-1d73-49a8-f300-a721acf565cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Generative AI ', metadata={'source': 'data/Generative AI.pdf', 'page': 0}),\n",
              " Document(page_content='What you will learn? \\n●Generative AI? \\n●Large Language Models (LLMs) \\n●OpenAI \\n●Langchain \\n●Vector Database \\n●Llama Index \\n●Open Source LLM model \\n●End to End Project ', metadata={'source': 'data/Generative AI.pdf', 'page': 1}),\n",
              " Document(page_content='Generative AI \\n●ChatGPT \\n●Google Bard \\n●Meta Llama 2 ', metadata={'source': 'data/Generative AI.pdf', 'page': 2}),\n",
              " Document(page_content='What is Generative AI? \\nGenerative AI generate new data based on training sample.Generative model \\ncan generate Image,Text, Audio, Videos etc. data as output. \\nSo generative AI is a very huge topics, \\n-Generative Image model \\n-Generative Language model ', metadata={'source': 'data/Generative AI.pdf', 'page': 3}),\n",
              " Document(page_content='Generative Model: \\nQuestions R esponses ', metadata={'source': 'data/Generative AI.pdf', 'page': 4}),\n",
              " Document(page_content='Where Generative AI Exists. \\n●Machine Learning is the subset of Artiﬁcial \\nIntelligence \\n●Deep Learning is the subset of Machine Learning \\n●Generative AI is the subset of Deep Learning ', metadata={'source': 'data/Generative AI.pdf', 'page': 5}),\n",
              " Document(page_content='Discriminative vs Generative Model \\n', metadata={'source': 'data/Generative AI.pdf', 'page': 6}),\n",
              " Document(page_content='Discriminative vs Generative Model \\n', metadata={'source': 'data/Generative AI.pdf', 'page': 7}),\n",
              " Document(page_content='Clustering: \\n●K-Means \\n●DBScan \\nClassiﬁcation & Regression: \\n', metadata={'source': 'data/Generative AI.pdf', 'page': 8}),\n",
              " Document(page_content=\"Generative AI is a subset of deep learning and Generative models are trained on huge amount \\nof data. While training the generative model we don’t need to provide a label data, It is not possible \\nwhen we have a huge amount of data, So, it's just try to see the relationship between the distribution \\nof the data. In Generative AI we give unstructured data to the LLM model for training purpose. \\n\", metadata={'source': 'data/Generative AI.pdf', 'page': 9}),\n",
              " Document(page_content='What is LLMs? \\nLarge Language Models (LLMs) are foundational machine learning models that use deep learning  \\nalgorithms to process and understand natural language. These models are trained on massive amounts  \\nof text data to learn patterns and entity relationships in the language.  \\nIt is a language  model which is responsible for performing task such as text to text generation  , text to  \\nimage generation  and image to text generations .', metadata={'source': 'data/Generative AI.pdf', 'page': 10}),\n",
              " Document(page_content='What makes LLM so Powerful? \\n●In case of LLM, one model can be used for a whole variety of tasks like:- \\nText generation, Chatbot, summarizer, translation, code generation \\n& so on … \\nSo, LLM is subset of Deep Learning & it has some properties merge with \\nGenerative AI ', metadata={'source': 'data/Generative AI.pdf', 'page': 11}),\n",
              " Document(page_content='', metadata={'source': 'data/Generative AI.pdf', 'page': 12}),\n",
              " Document(page_content='Thank You! ', metadata={'source': 'data/Generative AI.pdf', 'page': 13}),\n",
              " Document(page_content='Why LLM so Powerful? \\n●Train the model for a speciﬁc task \\n', metadata={'source': 'data/Generative AI.pdf', 'page': 14}),\n",
              " Document(page_content='Few milestone in large language model \\n●Bard \\n●GPT\\n●XLM \\n●T5\\n●M2M-100 ', metadata={'source': 'data/Generative AI.pdf', 'page': 15})]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import RecursiveCharacterTextSplitter for splitting documents into chunks.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Create a text splitter that splits text into chunks of 1000 characters with a 20 character overlap.\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "docs = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "Y4Oku2VU9PyD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the number of document chunks.\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdMExPvl9Je-",
        "outputId": "a60ddbfa-d4fe-49c7-e42c-2a63af8a979d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import userdata module for accessing user data in Google Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the OpenAI API key from user data.\n",
        "OPENAI_API_KEY = userdata.get('OPEN_AI_KEY')"
      ],
      "metadata": {
        "id": "yU1s4qem9hNr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import OpenAIEmbeddings to generate embeddings for documents.\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Initialize OpenAI embeddings with the API key.\n",
        "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "IjNaMEF_9X8i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Weaviate API key and cluster URL from user data.\n",
        "WEAVIATE_API_KEY = userdata.get('WEAVIATE_API_KEY')\n",
        "WEAVIATE_CLUSTER = userdata.get('WEAVIATE_CLUSTER')"
      ],
      "metadata": {
        "id": "_LOcJBu09_nk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Weaviate client and LangChain Weaviate vector store.\n",
        "import weaviate\n",
        "from langchain.vectorstores import Weaviate\n",
        "\n",
        "# Connect to the Weaviate cluster using the API key.\n",
        "auth_config = weaviate.auth.AuthApiKey(api_key = WEAVIATE_API_KEY)\n",
        "WEAVIATE_URL = WEAVIATE_CLUSTER\n",
        "\n",
        "client = weaviate.Client(\n",
        "    url = WEAVIATE_URL,\n",
        "    additional_headers = {\"X-OpenAI-Api-key\": OPENAI_API_KEY},\n",
        "    auth_client_secret = auth_config,\n",
        "    startup_period = 10\n",
        ")"
      ],
      "metadata": {
        "id": "wqZcTbZK9l0E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the Weaviate client is ready.\n",
        "client.is_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnOgUdEx-TxQ",
        "outputId": "789c8e20-8862-4c5a-f1b5-116f123d0b6b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete all existing schemas in the Weaviate cluster.\n",
        "client.schema.delete_all()\n",
        "client.schema.get()\n",
        "\n",
        "# Define a new schema for storing document vectors.\n",
        "schema = {\n",
        "    \"classes\": [\n",
        "        {\n",
        "            \"class\": \"Chatbot\",\n",
        "            \"description\": \"Documents for chatbot\",\n",
        "            \"vectorizer\": \"text2vec-openai\",\n",
        "            \"moduleConfig\": {\"text2vec-openai\": {\"model\": \"ada\", \"type\": \"text\"}},\n",
        "            \"properties\": [\n",
        "                {\n",
        "                    \"dataType\": [\"text\"],\n",
        "                    \"description\": \"The content of the paragraph\",\n",
        "                    \"moduleConfig\": {\n",
        "                        \"text2vec-openai\": {\n",
        "                            \"skip\": False,\n",
        "                            \"vectorizePropertyName\": False,\n",
        "                        }\n",
        "                    },\n",
        "                    \"name\": \"content\",\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create the schema in the Weaviate client.\n",
        "client.schema.create(schema)\n",
        "\n",
        "# Initialize the Weaviate vector store with the client, specifying the class and content property.\n",
        "vectorstore = Weaviate(client, \"Chatbot\", \"content\", attributes=[\"source\"])"
      ],
      "metadata": {
        "id": "eFq2-SBb-XRr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare text and metadata pairs for adding to the vector store.\n",
        "text_meta_pair = [(doc.page_content, doc.metadata) for doc in docs]\n",
        "texts, meta = list(zip(*text_meta_pair))\n",
        "\n",
        "# Add the texts and their metadata to the Weaviate vector store.\n",
        "vectorstore.add_texts(texts, meta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmfEK4w8-0Jl",
        "outputId": "95cecd2f-8aaa-4e62-a913-cc85829ddb22"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['024d0e8a-d276-4b6b-bd1e-9e53ec1de30c',\n",
              " '87b579f7-48a5-4755-b700-77b64dbbc283',\n",
              " '30b04ef6-e88e-4538-b3a0-1d2d7b1f0d6f',\n",
              " 'ead95a97-b028-4560-8999-4f7212413b56',\n",
              " '8e74112e-7c36-469b-9eab-cab737be5f9d',\n",
              " 'b76445ba-2200-45ef-a9e9-02584fa01732',\n",
              " '631676d4-17a9-4f02-a227-65e8fb35bc3f',\n",
              " '6e0f0ef0-85e3-4ef8-8521-14ea8467277a',\n",
              " '09a7f1f8-4d00-42c4-8fc9-5c928b162c9a',\n",
              " 'c2f3cef3-a864-40b8-85da-7cc56916e8e0',\n",
              " 'd2c29f15-a1bb-4df2-9ccf-0c3cc5efd0d2',\n",
              " '002be3ab-17d5-4bea-9199-d745e30ab76a',\n",
              " '547ec298-d179-4c83-872a-3c3c87030d2f',\n",
              " '5f0512b5-0c05-4587-b7a2-c6f395e9f820',\n",
              " '199fa0ef-286d-4123-959f-d85101fd3052']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a query to search for similar documents.\n",
        "query = \"What is LLM?\"\n",
        "\n",
        "# Perform a similarity search in the vector store with the query, retrieving the top 3 results.\n",
        "docs = vectorstore.similarity_search(query,top_k=3)"
      ],
      "metadata": {
        "id": "51jHhdre_CkK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the retrieved documents.\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1sE51PK_cFa",
        "outputId": "80bc028e-56be-424e-ba4b-3fb6d3bde2ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='What makes LLM so Powerful? \\n●In case of LLM, one model can be used for a whole variety of tasks like:- \\nText generation, Chatbot, summarizer, translation, code generation \\n& so on … \\nSo, LLM is subset of Deep Learning & it has some properties merge with \\nGenerative AI', metadata={'source': 'data/Generative AI.pdf'}),\n",
              " Document(page_content='What is LLMs? \\nLarge Language Models (LLMs) are foundational machine learning models that use deep learning  \\nalgorithms to process and understand natural language. These models are trained on massive amounts  \\nof text data to learn patterns and entity relationships in the language.  \\nIt is a language  model which is responsible for performing task such as text to text generation  , text to  \\nimage generation  and image to text generations .', metadata={'source': 'data/Generative AI.pdf'}),\n",
              " Document(page_content='Why LLM so Powerful? \\n●Train the model for a speciﬁc task', metadata={'source': 'data/Generative AI.pdf'}),\n",
              " Document(page_content='What you will learn? \\n●Generative AI? \\n●Large Language Models (LLMs) \\n●OpenAI \\n●Langchain \\n●Vector Database \\n●Llama Index \\n●Open Source LLM model \\n●End to End Project', metadata={'source': 'data/Generative AI.pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules for creating a RetrievalQA chain.\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "T86Yk0qg_ce0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI"
      ],
      "metadata": {
        "id": "btC2cWzNAFWF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the OpenAI language model with the API key.\n",
        "llm = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "HXtFM7inAcxK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RetrievalQA chain using the language model and the Weaviate vector store retriever.\n",
        "llm_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())"
      ],
      "metadata": {
        "id": "qmhrgwij_uxN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a query to be answered by the RetrievalQA chain.\n",
        "query = \"What is LLM?\"\n",
        "\n",
        "# Print the answer to the query by invoking the RetrievalQA chain.\n",
        "print(llm_chain.invoke(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKiEdJq2ALt8",
        "outputId": "eaee248a-003b-4f17-f666-a77b4a3388e6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'What is LLM?', 'result': ' Large Language Models (LLMs) are foundational machine learning models that use deep learning algorithms to process and understand natural language. They are trained on massive amounts of text data to learn patterns and relationships in language, making them powerful tools for tasks such as text generation, chatbots, summarization, translation, and code generation. LLMs are a subset of deep learning and have properties that merge with Generative AI.'}\n"
          ]
        }
      ]
    }
  ]
}