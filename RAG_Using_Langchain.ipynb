{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UvAHKYgNF2RR"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "! pip -q install langchain-openai langchain-community weaviate-client tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import requests library to fetch the text file from the URL\n",
        "import requests\n",
        "from langchain.document_loaders import TextLoader"
      ],
      "metadata": {
        "id": "9SpQV3FgIT-R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URL of the text file\n",
        "url = \"https://raw.githubusercontent.com/conwayyao/Recipe-Analysis/master/project_synopsis.txt\""
      ],
      "metadata": {
        "id": "BnJGCaboJWJ0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the content from the URL\n",
        "response = requests.get(url)"
      ],
      "metadata": {
        "id": "i66-4FSlJlDG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the content fetched from the URL\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnWbkQ91JqZI",
        "outputId": "e9264e2a-b3bf-445f-9d4b-0dd0da512aea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Why this project?\n",
            "\n",
            "Ironically, I enjoy cooking but I hate using recipes. In my opinion, recipes tend to encourage a slavish devotion to the recipe and divert attention from the more important part of cooking, the physical abilities (or, in my case, the lack thereof) of the chef. Foodies tend to privilege the provenance of obscure ingredients (\"coulis of feather saffron hand-picked from a seaside village in Morocco\"); I prefer the mundane but practical parts of cooking that get ignored in recipes (like freezing leftover sauce in ice cube trays, or the proper way to peel a mango).\n",
            "\n",
            "I am curious how much variation exists between dishes, and whether such variation is warranted. Are there really 5000 ways to cook a steak, or are many of these variations superfluous? Some chefs like Heston Blumenthal have taken an experimental approach to answering these questions, systematically and scientifically investigating every property of a dish, its ingredients, and its cooking methods to determine the \"best\" way to cook a dish. Since I do not have access to recipe instructions, I can only examine these recipes based on its ingredients. Nevertheless, I hope to use a data-science approach to see if the cooking wisdom of the crowds have arrived at the same answers, and if they match those of traditional experts.\n",
            "\n",
            "# Questions\n",
            "\n",
            "Dish explorer:\n",
            "\n",
            "1. What are the most common ingredients for a particular dish?\n",
            "2. What is the most unique recipe for each dish based on its ingredients?\n",
            "3. Given a recipe, which recipes are most similar to it? (Recipe comparison)\n",
            "4. Given a set of ingredients, which recipes can I make? (\"What's-in-the-fridge\" prediction)\n",
            "  - Which recipes can I make with a few more ingredients?\n",
            "6. Given a set of ingredients, which dish and recipe is it most like? (Dish/recipe classifier)\n",
            "\n",
            "Cuisine explorer:\n",
            "\n",
            "1. What are the most common ingredients for a particular cuisine?\n",
            "2. What is the most unique recipe for each cuisine based on its ingredients?\n",
            "3. Given a recipe, which recipes are most similar to it? (Recipe comparison)\n",
            "4. Given a set of ingredients, which recipes can I make? (\"What's-in-the-fridge\" prediction)\n",
            "  - Which recipes can I make with a few more ingredients?\n",
            "6. Given a set of ingredients, which dish and recipe is it most like? (Dish/recipe classifier)\n",
            "\n",
            "Ratings and general exploration:\n",
            "\n",
            "1. Is there a relationship between recipe rating and number of ingredients?\n",
            "2. Is there a relationship between recipe rating and recipe cost?\n",
            "4. What other interesting relationships exist in the data?\n",
            "\n",
            "# Data Collection\n",
            "\n",
            "In my preliminary project discussion, I had chosen to use the Spoonacular API, whose team had granted me an academic usage key. Spoonacular's API was generally very satisfactory but I had concerns with the number and quality of recipes on their website, and the quality of the ingredients provided for each recipe. A week after the preliminary project presentation, I received academic usage approval for Yummly's recipe API. While Spoonacular's API had more bells and whistles and advanced features, I believed that Yummly's database of recipes was generally of higher quality than Spoonacular's. It also had access to over 1 million recipes compared to Spoonacular's, and had better-formatted ingredients for each recipe. For these advantages, I switched data collection from Spoonacular to Yummly's API.\n",
            "\n",
            "Yummly's API is accessed by querying an API endpoint and returns JSON-encoded recipe data. It was easy to set up and use. Since I divided the project into two parallel paths (cuisine and dish), the search queries were straightforward: either search by dish name (e.g. \"burger\") or specify a search by cuisine (e.g. cuisine-Chinese). JSON data was translated to Pandas dataframe structures using the Requests package and Pandas.\n",
            "\n",
            "I encountered difficulties with the API when I tried to retrieve too many recipes at once. Initially, I hoped to create a list of all cuisines or dishes and loop through those lists, requesting recipes during each loop. Unfortunately, the loop would break after a few cuisines or dishes due to problems on Yummy's end. I opted to write separate CSV files for each dish and cuisine and concatenate them together into a master cuisines.csv and a master dishes.csv at the end rather than try to do it all in one go.\n",
            "\n",
            "In total I made 454 calls to Yummly's API over four days of data collection (accounting for missteps and experimentation). I wrote cuisine data into 25 separate CSVs before assembling them into cuisines_data.csv, and wrote dishes data into 45 separate CSVs before assembling into dishes_data.csv.\n",
            "\n",
            "Each row of data contained:\n",
            "\n",
            "- Yummly Recipe ID (string)\n",
            "- Recipe Name (string)\n",
            "- Yummly Rating (integer, 0-5)\n",
            "- Cooking Time in Seconds (integer)\n",
            "- Course (string)\n",
            "- Cuisine (string)\n",
            "- Ingredients (string)\n",
            "\n",
            "# Data Pre-Processing\n",
            "\n",
            "I conducted several processing steps on the two master data CSVs (dishes and cuisines):\n",
            "\n",
            "1. Ingredient parsing:\n",
            "\n",
            "\tEach recipe's ingredients were encoded as a single, unseparated string, including brackets: \"[ingredient 1, ingredient 2, ingredient 3]\". Since I wanted to examine each ingredient separately, I dropped the brackets from each string by reading from [1:-1]. Next, I separated by comma, then returned a list of these ingredient strings.\n",
            "\n",
            "\tYummly's ingredient specifications are imperfect. A human would be able to recognize that \"fresh pasta\", \"pasta\", and \"Giorgino pasta\" are essentially the same ingredient, but this code necessarily treats these as three separate and unique ingredients.\n",
            "\n",
            "2. Ingredient counts:\n",
            "\n",
            "\tAfter parsing the ingredients from a single string to a list of strings, it was easy to calculate the number of ingredients in each recipe by creating a new column named \"ingredCount\" and setting each value to len(ingredients).\n",
            "\n",
            "3. Time conversion:\n",
            "\n",
            "\tAs we are more accustomed to thinking about cooking times in minutes rather than seconds, I converted each recipe's cooking time from seconds to minutes by dividing by 60 and populated a column named \"timeMins\".\n",
            "\n",
            "4. Munging:\n",
            "\n",
            "\tI conducted an initial round of munging to ensure my data was sufficiently clean for analysis. \n",
            "\n",
            "\tThe most important one was to drop any rows of cuisines data that had an empty \"cuisine\" value. Yummly's \"Search by Cuisine\" API call returns recipes that have \"Chinese\" in the recipe in some shape or form-- even in the ingredients! So a sandwich recipe that employs \"French bread\" or an \"English muffin\" might have a cuisine value of \"French\" or \"English\". To avoid these ambiguities, I dropped any empty recipes without an explicit cuisine. This greatly reduced the number of recipes for some cuisines like English. I only did this for my cuisines_data.csv; since dishes_data.csv is unconcerned with dish origin, there was no need to drop dishes with empty cuisine values.\n",
            "\n",
            "\tFor dishes_data.csv, I explicitly assigned each recipe to the search query that it was gathered from, using a new column named \"dish\". For example, all recipes from the burger.csv file were appended with \"dish=burger\". This dramatically simplified later analyses at the expense of some accuracy. I could not do this with cuisines_data, because a lot of the belonged to multiple cuisines and I wanted to preserve this complexity.\n",
            "\n",
            "\tFor both dishes_data and cuisines_data, I filled in empty course values with \"Unknown\" rather than dropping these recipes.\n",
            "\n",
            "\tFinally, I dropped duplicate recipes from both datasets. For example, a recipe for pad thai that is listed as both \"Asian\" and \"Thai\" within its cuisine value will show up twice in the dataset: first during the search for Asian dishes, then again during the search for Thai dishes.\n",
            "\n",
            "# Data Analysis:\n",
            "\n",
            "## Dish Dataset:\n",
            "\n",
            "Dish dataset basic stats:\n",
            "\n",
            "- 16402 recipes\n",
            "- 44 dishes (e.g. burger, burrito, steak, gumbo)\n",
            "  - Dishes with the most recipes: cake (633), chowder (545), pancakes (457), roast chicken (444)\n",
            "  - Dishes with the least recipes: donut (202), tacos (221), chili (265), turkey (275)\n",
            "- 25 Cuisines: American is the most well-represented, with 396 recipes. Portuguese the least, with just 2 recipes\n",
            "- Ratings: overwhelmingly 4-star (10984 of the 16402), some 3-star (3646) and 5-star (1683), very few 0, 1, or 2 stars (89 combined recipes)\n",
            "- Cooking Time: mean of 65 minutes, max of 5760 minutes (4 days), min of 1 minute, median of 45 minutes\n",
            "- Ingredients per recipe: mean of 9.9, max of 59 (swordfish ceviche), min of 1, median of 9\n",
            "- 5385 unique ingredients:\n",
            "  - Most common ingredients: Salt (6978 occurences, 0.43 frequency), butter (0.21), eggs (0.17), sugar (0.16), onions (0.15)\n",
            "  - Least common ingredients: 1675 unique ingredients are only used in one recipe throughout the dataset. Examples: buckwheat noodles, gluten-free pie crust, canned snails, sugar-free Jell-O gelatin\n",
            "\n",
            "## Cuisine Dataset:\n",
            "\n",
            "Cuisine dataset basic stats:\n",
            "\n",
            "- 8037 recipes\n",
            "- 25 cuisines: Asian is the most well-represented, with 1414 recipes. English the least, with just 32 recipes.\n",
            "- Ratings: overwhelmingly 4-star (4988 of the 8037), some 3-star (1517) and 5-star (1379), very few 0, 1, or 2 stars (153 combined recipes)\n",
            "- Cooking Time: mean of 65 minutes, max of 1970 minutes (33 hours), min of 1 minute, median of 40 minutes\n",
            "- Ingredients per recipe: mean of 10.1, max of 35 (Thai chicken tacos), min of 1, median of 9\n",
            "- 3958 unique ingredients:\n",
            "  - Most common ingredients: Salt (3325 occurences, 0.41 frequency), garlic (0.22), onions (0.20), olive oil (0.18)\n",
            "  - Least common ingredients: 1401 unique ingredients are only used in one recipe throughout the dataset. Examples: chocolate candy, melon seeds, canned tuna, vegan yogurt, tarragon vinegar, unsalted almonds\n",
            "\n",
            "## Unique Ingredient Analysis:\n",
            "\n",
            "I wanted to examine the frequency and type of unique ingredients employed by each cuisine or each dish. I created a function with an input of a DataFrame that iterates through each recipe in the DataFrame and adds the contents of each recipe's ingredients list to a summation list. This list is converted to a Pandas Series so that I can take advantage of Pandas' value_counts() method to count each appearance of an ingredient. The output of the value_counts() is saved as a column named \"instances\" representing the number of recipes that the ingredient appears in. I then calculated ingredient frequency by dividing the instances by the number of recipes in the dataset, and appended as a new column \"frequency\".\n",
            "\n",
            "Since this function takes a DataFrame as an input, it can be called on any dish, cuisine, or subset of dishes or cuisines to examine their unique ingredient counts and frequencies.\n",
            "\n",
            "From just cursory examination, it is easy to see how cuisines cook with different ingredients, and see which cuisines are similar to each other. For example, here are the most popular ingredients for some cuisines:\n",
            "\n",
            "- American: salt, butter, all-purpose flour, sugar, olive oil, water, onions, pepepr, unsalted butter, coarse salt\n",
            "- French: salt, unsalted butter, sugar, butter, all-purpose flour, water, eggs, heavy cream, milk\n",
            "- Chinese: soy sauce, corn starch, salt, sesame oil, garlic, sugar, water, scallions, ginger, oil\n",
            "- Indian: salt, onions, garam masala, cumin seed, ground turmeric, garlic, ground cumin, oil, ginger, water\n",
            "- Italian: salt, olive oil, parmesan cheese, garlic, extra-virgin olive oil, onions, garlic, pepper, eggs\n",
            "- Thai: fish sauce, coconut milk, garlic, lime, soy sauce, salt, lime juice, vegetable oil, brown sugar\n",
            "\n",
            "It is easy to see that American and French cuisine have many similarities, with frequent use of butter, sugar, flour, and salt. Chinese, Thai, and Indian cooking uses completely separate palettes of ingredients. This answers one of my questions, what are the most indicative ingredients for each cuisine?\n",
            "\n",
            "## Recipe Uniqueness Scoring:\n",
            "\n",
            "Next, I identified which recipes within a given dataset are the most 'unique' in terms of their ingredients. For example, one expects that most chili recipes will contain tomato sauce, beans, ground beef, and onions. If a chili recipe does not use any of these ingredients and instead uses fruits or obscure meats, it is very different from the norm and should receive a high uniqueness score.\n",
            "\n",
            "The first method of scoring is to take the mean of each recipe's ingredient frequencies (e.g. (0.4+0.2+0.1) / 3 ). For each recipe's ingredients, I sum the ingredients' unique ingredient frequency score calculated above, then divide by the number of ingredients in the recipe so as to not bias the scoring towards recipes with dozens of ingredients. I assign this score to a column named \"uniq_score1\".\n",
            "\n",
            "The second method of scoring is to take the product of each recipe's ingredient frequencies (e.g. 0.4 * 0.2 * 0.1). I assign this score to a column named \"uniq_score2\".\n",
            "\n",
            "Both methods have their pros and cons and some more thinking is required to assess which better fits our intuition. Both methods produce broadly similar results, but the mean score seems to match our intuition slightly better than the product. The product tends to 'reward' recipes with extremely rare ingredients whereas the mean method 'rewards' these recipes to far lower extent.\n",
            "\n",
            "Nevertheless, by examining the results I am more or less satisfied with these scoring methods. The most 'typical' American Main Course, for example, is Southern Fried Chicken, which uses salt, butter, chicken, and oil. An extremely 'atypical' dish is the swordfish ceviche, which uses 59 ingredients that are each very rare.\n",
            "\n",
            "## Data Relationships:\n",
            "\n",
            "I was disappointed by the inability to discover any meaningful relationships between some key metrics. For example, I hypothesized that recipes with extremely long (arduous) or extremely short (too simple) cooking times would receive lower ratings than recipes with a reasonable cooking time. Likewise recipes with many ingredients or very few ingredients. However, the ratings provided by Yummly were extremely uneven and extremely unlikely, with more than half the recipes in both cuisines_data and dishes_data holding a rating of 4. Almost no 0, 1, or 2-star recipes exist in either dataset, suggesting that Yummly is cooking the rating data somehow like Fandango's movie ratings. \n",
            "\n",
            "After plotting cooking time vs. ratings and ingredient counts vs. ratings, it was clear that there are no correlations because of the incompleteness of the underlying ratings. \n",
            "\n",
            "There may or may not be a relationship between ingredient counts and cooking times. According to the scatterplot it does not appear linear but rather bell-shaped. I did not yet fit a multidimensional regression curve to it yet.\n",
            "\n",
            "# Data Dictionary:\n",
            "\n",
            "My final data dictionary:\n",
            "\n",
            "- Recipe ID ('id', string): set as index of both dishes_data.csv and cuisines_data.csv\n",
            "- Recipe Name ('recipeName', string)\n",
            "- Cooking Time in Seconds ('totalTimeInSeconds', integer)\n",
            "- Cooking Time in Minutes ('timeMins', integer): calculated during processing stage\n",
            "- Yummly Rating ('rating', integer): ratings from 0 to 5\n",
            "- Course ('course', string): \"Unknown\" filled in for null values during processing stage\n",
            "- Cuisine ('cuisine', string)\n",
            "- Dish ('dish', string): for dishes_data.csv ONLY\n",
            "- Ingredients ('ingredients', list of strings): parsed during processing stage\n",
            "- Ingredient Count ('ingred_count', integer): calculated during processing stage\n",
            "- Uniqueness Score #1 ('uniq_score1', floating): calculated during analysis stage; mean of ingredient frequencies\n",
            "- Uniqueness Score #2 ('uniq_score2', floating): calculated during analysis stage; product of ingredient frequencies\n",
            "- Can I Make This Dish? ('possible', boolean): determined during \"What Can I Make?\" analysis\n",
            "\n",
            "Unique ingredient data dictionary:\n",
            "\n",
            "- Ingredient name ('ingredient', string): set as index of uniq_ingred DataFrame\n",
            "- Count ('instances', integer): number of recipes that the ingredient appears in\n",
            "- Frequency ('frequency', floating): percentage of recipes that the ingredient appears in; calculated by dividing count by the number of recipes in the dataset\n",
            "\n",
            "# What Can I Make?\n",
            "\n",
            "In order to find which recipes are possible given a set of supplies in a pantry, I employed Python's .issubset() method, which determines if all of the objects in one set (the recipe) exist in another set (your pantry). I created a function which iterates through a dataset's recipes and sets a boolean named \"possible\" to True if all ingredients in the recipe exist in your pantry. Again, the ingredients were parsed naively; the code has no way of knowing that \"fresh basil leaves\" can satisfy a requirement for \"basil leaves\". The computer treats these as two distinct ingredients.\n",
            "\n",
            "Rather than type out a long list of items in a pantry, I decided the logical way for someone to stock a pantry would be to purchase the most common unique ingredients for whichever cuisine or dish the person was interested in making. For both cuisines_data and dishes_data, I stocked the pantry with the 100 most common unique ingredients in the dataset, then ran my function to see how many dishes were possible.\n",
            "\n",
            "I was suprised to see how few dishes were possible even with a well-stocked pantry. With the 100 most common unique ingredients, one can only make 114 out of the 8037 recipes in the cuisines_data dataset. To explore how adding additional ingredients to your pantry increased the number of recipes you can make, I successively added to the pantry and plotted how many recipes were possible. Surprisingly, the relationship is fairly linear, with an R^2 of 0.994, at least up to the 1000 most common ingredients (due to running time, I did not extend all the way to 3958 ingredients). With 1000 of the most common ingredients, one can make roughly 3500 recipes. The curve does show a slight logistic tendency, as would be expected: adding ingredients rapidly expands the number of recipes you can make before petering out.\n",
            "\n",
            "# Machine Learning and Classification\n",
            "\n",
            "In order to add a predictive component to this project, I utilized machine learning to predict the cuisine or dish of a recipe given its ingredients. Since we have established that cuisines differ substantially in terms of their ingredient usages, I expected fairly good results for these Cuisine and Dish classifiers.\n",
            "\n",
            "For each recipe, I converted the ingredients of each recipe from a list of strings back to a single string so that I could utilize the bag-of-words model. This is admittedly a very inelegant solution, as there should exist a method to use the ingredients as the tokens for a model rather than deconvert back to a bag-of-words. Then I train-test-split with sklearn and fit a Naive Bayes and Logistic Regression model. I compared the predicted cuisines or dishes with the real cuisines and dishes and achieved surprisingly accurate results out-of-the-box:\n",
            "\n",
            "Null accuracy of cuisine prediction is \"Asian\", 555/8037 = 0.06906:\n",
            "\n",
            "- Multinomial Naive Bayes: 0.53383 accuracy. 7x better than null accuracy\n",
            "- Logistic Regression: 0.43881 accuracy, 6x better tha null accuracy\n",
            "\n",
            "Null accuracy of dish prediction is \"Cake\", 633/16402 = 0.038592:\n",
            "\n",
            "- Multinomial Naive Bayes: 0.71080, 18x better than null accuracy\n",
            "- Logistic Regression: 0.70299, 18x better than null accuracy\n",
            "\n",
            "A major reason why cuisine prediction was less accurate than dish prediction was that the cuisine data was not encoded as cleanly. Every recipe in the dishes_data dataset was assigned an explicit \"dish\" value, whereas every recipe in the cuisines_data dataset was NOT assigned an explicit cuisine. Many of the recipes in the cuisines_data dataset had multiple listings under cuisine. For example, a recipe could be listed as \"[Asian, Chinese]\". When previously counting the number of recipes that were Asian, I boolean-filtered for all recipes that had \"Asian\" within the string of \"cuisine\". So this recipe would have been counted twice, once for \"Asian\" and once for \"Chinese\"-- hence the total number of recipes does not add up.\n",
            "\n",
            "When doing this classification for cuisines, there is no easy way to disaggregate Asian and Chinese, as the computer must decide which of the two to classify any one recipe. So I kept all multi-cuisine designations rather than try to impute one over another. Rather than 25 cuisines to predict, I had 206, many combos of which only existed once in the dataset (e.g. \"[Barbecue, Mediterranean, Greek]\" or \"[Italian, Japanese]\"). Given this messiness, I was very pleased that the overall accuracy was still 7x better than null.\n",
            "\n",
            "# Next Steps\n",
            "\n",
            "My next steps include:\n",
            "\n",
            "1. Refining cuisine and dish prediction. I only did minimal experimentation with model hyperparameters. I want to revisit the multiple-cuisine designation problem and ingredient-tokenization problem I mentioned above. I believe I can improve accuracy by another 15% for cuisines prediction, and perhaps another 10% for dish prediction.\n",
            "\n",
            "2. Publishing interesting results or infographics online, using static images or Bokeh/ D3\n",
            "\n",
            "3. Clustering analysis\n",
            "\n",
            "3. Pricing analysis if I can find an API that accepts ingredients and returns price per serving.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fetched content to a local file\n",
        "with open(\"project_synopsis.txt\",\"w\") as f:\n",
        "  f.write(response.text)"
      ],
      "metadata": {
        "id": "y_-iZEu0Lwwy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the text file into a document object\n",
        "loader = TextLoader(\"project_synopsis.txt\")"
      ],
      "metadata": {
        "id": "3lEssbZpMDEF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the document\n",
        "document = loader.load()\n",
        "print(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLLhDec2MzAi",
        "outputId": "657c7ab4-0053-4963-9639-0af364dbdd91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='# Why this project?\\n\\nIronically, I enjoy cooking but I hate using recipes. In my opinion, recipes tend to encourage a slavish devotion to the recipe and divert attention from the more important part of cooking, the physical abilities (or, in my case, the lack thereof) of the chef. Foodies tend to privilege the provenance of obscure ingredients (\"coulis of feather saffron hand-picked from a seaside village in Morocco\"); I prefer the mundane but practical parts of cooking that get ignored in recipes (like freezing leftover sauce in ice cube trays, or the proper way to peel a mango).\\n\\nI am curious how much variation exists between dishes, and whether such variation is warranted. Are there really 5000 ways to cook a steak, or are many of these variations superfluous? Some chefs like Heston Blumenthal have taken an experimental approach to answering these questions, systematically and scientifically investigating every property of a dish, its ingredients, and its cooking methods to determine the \"best\" way to cook a dish. Since I do not have access to recipe instructions, I can only examine these recipes based on its ingredients. Nevertheless, I hope to use a data-science approach to see if the cooking wisdom of the crowds have arrived at the same answers, and if they match those of traditional experts.\\n\\n# Questions\\n\\nDish explorer:\\n\\n1. What are the most common ingredients for a particular dish?\\n2. What is the most unique recipe for each dish based on its ingredients?\\n3. Given a recipe, which recipes are most similar to it? (Recipe comparison)\\n4. Given a set of ingredients, which recipes can I make? (\"What\\'s-in-the-fridge\" prediction)\\n  - Which recipes can I make with a few more ingredients?\\n6. Given a set of ingredients, which dish and recipe is it most like? (Dish/recipe classifier)\\n\\nCuisine explorer:\\n\\n1. What are the most common ingredients for a particular cuisine?\\n2. What is the most unique recipe for each cuisine based on its ingredients?\\n3. Given a recipe, which recipes are most similar to it? (Recipe comparison)\\n4. Given a set of ingredients, which recipes can I make? (\"What\\'s-in-the-fridge\" prediction)\\n  - Which recipes can I make with a few more ingredients?\\n6. Given a set of ingredients, which dish and recipe is it most like? (Dish/recipe classifier)\\n\\nRatings and general exploration:\\n\\n1. Is there a relationship between recipe rating and number of ingredients?\\n2. Is there a relationship between recipe rating and recipe cost?\\n4. What other interesting relationships exist in the data?\\n\\n# Data Collection\\n\\nIn my preliminary project discussion, I had chosen to use the Spoonacular API, whose team had granted me an academic usage key. Spoonacular\\'s API was generally very satisfactory but I had concerns with the number and quality of recipes on their website, and the quality of the ingredients provided for each recipe. A week after the preliminary project presentation, I received academic usage approval for Yummly\\'s recipe API. While Spoonacular\\'s API had more bells and whistles and advanced features, I believed that Yummly\\'s database of recipes was generally of higher quality than Spoonacular\\'s. It also had access to over 1 million recipes compared to Spoonacular\\'s, and had better-formatted ingredients for each recipe. For these advantages, I switched data collection from Spoonacular to Yummly\\'s API.\\n\\nYummly\\'s API is accessed by querying an API endpoint and returns JSON-encoded recipe data. It was easy to set up and use. Since I divided the project into two parallel paths (cuisine and dish), the search queries were straightforward: either search by dish name (e.g. \"burger\") or specify a search by cuisine (e.g. cuisine-Chinese). JSON data was translated to Pandas dataframe structures using the Requests package and Pandas.\\n\\nI encountered difficulties with the API when I tried to retrieve too many recipes at once. Initially, I hoped to create a list of all cuisines or dishes and loop through those lists, requesting recipes during each loop. Unfortunately, the loop would break after a few cuisines or dishes due to problems on Yummy\\'s end. I opted to write separate CSV files for each dish and cuisine and concatenate them together into a master cuisines.csv and a master dishes.csv at the end rather than try to do it all in one go.\\n\\nIn total I made 454 calls to Yummly\\'s API over four days of data collection (accounting for missteps and experimentation). I wrote cuisine data into 25 separate CSVs before assembling them into cuisines_data.csv, and wrote dishes data into 45 separate CSVs before assembling into dishes_data.csv.\\n\\nEach row of data contained:\\n\\n- Yummly Recipe ID (string)\\n- Recipe Name (string)\\n- Yummly Rating (integer, 0-5)\\n- Cooking Time in Seconds (integer)\\n- Course (string)\\n- Cuisine (string)\\n- Ingredients (string)\\n\\n# Data Pre-Processing\\n\\nI conducted several processing steps on the two master data CSVs (dishes and cuisines):\\n\\n1. Ingredient parsing:\\n\\n\\tEach recipe\\'s ingredients were encoded as a single, unseparated string, including brackets: \"[ingredient 1, ingredient 2, ingredient 3]\". Since I wanted to examine each ingredient separately, I dropped the brackets from each string by reading from [1:-1]. Next, I separated by comma, then returned a list of these ingredient strings.\\n\\n\\tYummly\\'s ingredient specifications are imperfect. A human would be able to recognize that \"fresh pasta\", \"pasta\", and \"Giorgino pasta\" are essentially the same ingredient, but this code necessarily treats these as three separate and unique ingredients.\\n\\n2. Ingredient counts:\\n\\n\\tAfter parsing the ingredients from a single string to a list of strings, it was easy to calculate the number of ingredients in each recipe by creating a new column named \"ingredCount\" and setting each value to len(ingredients).\\n\\n3. Time conversion:\\n\\n\\tAs we are more accustomed to thinking about cooking times in minutes rather than seconds, I converted each recipe\\'s cooking time from seconds to minutes by dividing by 60 and populated a column named \"timeMins\".\\n\\n4. Munging:\\n\\n\\tI conducted an initial round of munging to ensure my data was sufficiently clean for analysis. \\n\\n\\tThe most important one was to drop any rows of cuisines data that had an empty \"cuisine\" value. Yummly\\'s \"Search by Cuisine\" API call returns recipes that have \"Chinese\" in the recipe in some shape or form-- even in the ingredients! So a sandwich recipe that employs \"French bread\" or an \"English muffin\" might have a cuisine value of \"French\" or \"English\". To avoid these ambiguities, I dropped any empty recipes without an explicit cuisine. This greatly reduced the number of recipes for some cuisines like English. I only did this for my cuisines_data.csv; since dishes_data.csv is unconcerned with dish origin, there was no need to drop dishes with empty cuisine values.\\n\\n\\tFor dishes_data.csv, I explicitly assigned each recipe to the search query that it was gathered from, using a new column named \"dish\". For example, all recipes from the burger.csv file were appended with \"dish=burger\". This dramatically simplified later analyses at the expense of some accuracy. I could not do this with cuisines_data, because a lot of the belonged to multiple cuisines and I wanted to preserve this complexity.\\n\\n\\tFor both dishes_data and cuisines_data, I filled in empty course values with \"Unknown\" rather than dropping these recipes.\\n\\n\\tFinally, I dropped duplicate recipes from both datasets. For example, a recipe for pad thai that is listed as both \"Asian\" and \"Thai\" within its cuisine value will show up twice in the dataset: first during the search for Asian dishes, then again during the search for Thai dishes.\\n\\n# Data Analysis:\\n\\n## Dish Dataset:\\n\\nDish dataset basic stats:\\n\\n- 16402 recipes\\n- 44 dishes (e.g. burger, burrito, steak, gumbo)\\n  - Dishes with the most recipes: cake (633), chowder (545), pancakes (457), roast chicken (444)\\n  - Dishes with the least recipes: donut (202), tacos (221), chili (265), turkey (275)\\n- 25 Cuisines: American is the most well-represented, with 396 recipes. Portuguese the least, with just 2 recipes\\n- Ratings: overwhelmingly 4-star (10984 of the 16402), some 3-star (3646) and 5-star (1683), very few 0, 1, or 2 stars (89 combined recipes)\\n- Cooking Time: mean of 65 minutes, max of 5760 minutes (4 days), min of 1 minute, median of 45 minutes\\n- Ingredients per recipe: mean of 9.9, max of 59 (swordfish ceviche), min of 1, median of 9\\n- 5385 unique ingredients:\\n  - Most common ingredients: Salt (6978 occurences, 0.43 frequency), butter (0.21), eggs (0.17), sugar (0.16), onions (0.15)\\n  - Least common ingredients: 1675 unique ingredients are only used in one recipe throughout the dataset. Examples: buckwheat noodles, gluten-free pie crust, canned snails, sugar-free Jell-O gelatin\\n\\n## Cuisine Dataset:\\n\\nCuisine dataset basic stats:\\n\\n- 8037 recipes\\n- 25 cuisines: Asian is the most well-represented, with 1414 recipes. English the least, with just 32 recipes.\\n- Ratings: overwhelmingly 4-star (4988 of the 8037), some 3-star (1517) and 5-star (1379), very few 0, 1, or 2 stars (153 combined recipes)\\n- Cooking Time: mean of 65 minutes, max of 1970 minutes (33 hours), min of 1 minute, median of 40 minutes\\n- Ingredients per recipe: mean of 10.1, max of 35 (Thai chicken tacos), min of 1, median of 9\\n- 3958 unique ingredients:\\n  - Most common ingredients: Salt (3325 occurences, 0.41 frequency), garlic (0.22), onions (0.20), olive oil (0.18)\\n  - Least common ingredients: 1401 unique ingredients are only used in one recipe throughout the dataset. Examples: chocolate candy, melon seeds, canned tuna, vegan yogurt, tarragon vinegar, unsalted almonds\\n\\n## Unique Ingredient Analysis:\\n\\nI wanted to examine the frequency and type of unique ingredients employed by each cuisine or each dish. I created a function with an input of a DataFrame that iterates through each recipe in the DataFrame and adds the contents of each recipe\\'s ingredients list to a summation list. This list is converted to a Pandas Series so that I can take advantage of Pandas\\' value_counts() method to count each appearance of an ingredient. The output of the value_counts() is saved as a column named \"instances\" representing the number of recipes that the ingredient appears in. I then calculated ingredient frequency by dividing the instances by the number of recipes in the dataset, and appended as a new column \"frequency\".\\n\\nSince this function takes a DataFrame as an input, it can be called on any dish, cuisine, or subset of dishes or cuisines to examine their unique ingredient counts and frequencies.\\n\\nFrom just cursory examination, it is easy to see how cuisines cook with different ingredients, and see which cuisines are similar to each other. For example, here are the most popular ingredients for some cuisines:\\n\\n- American: salt, butter, all-purpose flour, sugar, olive oil, water, onions, pepepr, unsalted butter, coarse salt\\n- French: salt, unsalted butter, sugar, butter, all-purpose flour, water, eggs, heavy cream, milk\\n- Chinese: soy sauce, corn starch, salt, sesame oil, garlic, sugar, water, scallions, ginger, oil\\n- Indian: salt, onions, garam masala, cumin seed, ground turmeric, garlic, ground cumin, oil, ginger, water\\n- Italian: salt, olive oil, parmesan cheese, garlic, extra-virgin olive oil, onions, garlic, pepper, eggs\\n- Thai: fish sauce, coconut milk, garlic, lime, soy sauce, salt, lime juice, vegetable oil, brown sugar\\n\\nIt is easy to see that American and French cuisine have many similarities, with frequent use of butter, sugar, flour, and salt. Chinese, Thai, and Indian cooking uses completely separate palettes of ingredients. This answers one of my questions, what are the most indicative ingredients for each cuisine?\\n\\n## Recipe Uniqueness Scoring:\\n\\nNext, I identified which recipes within a given dataset are the most \\'unique\\' in terms of their ingredients. For example, one expects that most chili recipes will contain tomato sauce, beans, ground beef, and onions. If a chili recipe does not use any of these ingredients and instead uses fruits or obscure meats, it is very different from the norm and should receive a high uniqueness score.\\n\\nThe first method of scoring is to take the mean of each recipe\\'s ingredient frequencies (e.g. (0.4+0.2+0.1) / 3 ). For each recipe\\'s ingredients, I sum the ingredients\\' unique ingredient frequency score calculated above, then divide by the number of ingredients in the recipe so as to not bias the scoring towards recipes with dozens of ingredients. I assign this score to a column named \"uniq_score1\".\\n\\nThe second method of scoring is to take the product of each recipe\\'s ingredient frequencies (e.g. 0.4 * 0.2 * 0.1). I assign this score to a column named \"uniq_score2\".\\n\\nBoth methods have their pros and cons and some more thinking is required to assess which better fits our intuition. Both methods produce broadly similar results, but the mean score seems to match our intuition slightly better than the product. The product tends to \\'reward\\' recipes with extremely rare ingredients whereas the mean method \\'rewards\\' these recipes to far lower extent.\\n\\nNevertheless, by examining the results I am more or less satisfied with these scoring methods. The most \\'typical\\' American Main Course, for example, is Southern Fried Chicken, which uses salt, butter, chicken, and oil. An extremely \\'atypical\\' dish is the swordfish ceviche, which uses 59 ingredients that are each very rare.\\n\\n## Data Relationships:\\n\\nI was disappointed by the inability to discover any meaningful relationships between some key metrics. For example, I hypothesized that recipes with extremely long (arduous) or extremely short (too simple) cooking times would receive lower ratings than recipes with a reasonable cooking time. Likewise recipes with many ingredients or very few ingredients. However, the ratings provided by Yummly were extremely uneven and extremely unlikely, with more than half the recipes in both cuisines_data and dishes_data holding a rating of 4. Almost no 0, 1, or 2-star recipes exist in either dataset, suggesting that Yummly is cooking the rating data somehow like Fandango\\'s movie ratings. \\n\\nAfter plotting cooking time vs. ratings and ingredient counts vs. ratings, it was clear that there are no correlations because of the incompleteness of the underlying ratings. \\n\\nThere may or may not be a relationship between ingredient counts and cooking times. According to the scatterplot it does not appear linear but rather bell-shaped. I did not yet fit a multidimensional regression curve to it yet.\\n\\n# Data Dictionary:\\n\\nMy final data dictionary:\\n\\n- Recipe ID (\\'id\\', string): set as index of both dishes_data.csv and cuisines_data.csv\\n- Recipe Name (\\'recipeName\\', string)\\n- Cooking Time in Seconds (\\'totalTimeInSeconds\\', integer)\\n- Cooking Time in Minutes (\\'timeMins\\', integer): calculated during processing stage\\n- Yummly Rating (\\'rating\\', integer): ratings from 0 to 5\\n- Course (\\'course\\', string): \"Unknown\" filled in for null values during processing stage\\n- Cuisine (\\'cuisine\\', string)\\n- Dish (\\'dish\\', string): for dishes_data.csv ONLY\\n- Ingredients (\\'ingredients\\', list of strings): parsed during processing stage\\n- Ingredient Count (\\'ingred_count\\', integer): calculated during processing stage\\n- Uniqueness Score #1 (\\'uniq_score1\\', floating): calculated during analysis stage; mean of ingredient frequencies\\n- Uniqueness Score #2 (\\'uniq_score2\\', floating): calculated during analysis stage; product of ingredient frequencies\\n- Can I Make This Dish? (\\'possible\\', boolean): determined during \"What Can I Make?\" analysis\\n\\nUnique ingredient data dictionary:\\n\\n- Ingredient name (\\'ingredient\\', string): set as index of uniq_ingred DataFrame\\n- Count (\\'instances\\', integer): number of recipes that the ingredient appears in\\n- Frequency (\\'frequency\\', floating): percentage of recipes that the ingredient appears in; calculated by dividing count by the number of recipes in the dataset\\n\\n# What Can I Make?\\n\\nIn order to find which recipes are possible given a set of supplies in a pantry, I employed Python\\'s .issubset() method, which determines if all of the objects in one set (the recipe) exist in another set (your pantry). I created a function which iterates through a dataset\\'s recipes and sets a boolean named \"possible\" to True if all ingredients in the recipe exist in your pantry. Again, the ingredients were parsed naively; the code has no way of knowing that \"fresh basil leaves\" can satisfy a requirement for \"basil leaves\". The computer treats these as two distinct ingredients.\\n\\nRather than type out a long list of items in a pantry, I decided the logical way for someone to stock a pantry would be to purchase the most common unique ingredients for whichever cuisine or dish the person was interested in making. For both cuisines_data and dishes_data, I stocked the pantry with the 100 most common unique ingredients in the dataset, then ran my function to see how many dishes were possible.\\n\\nI was suprised to see how few dishes were possible even with a well-stocked pantry. With the 100 most common unique ingredients, one can only make 114 out of the 8037 recipes in the cuisines_data dataset. To explore how adding additional ingredients to your pantry increased the number of recipes you can make, I successively added to the pantry and plotted how many recipes were possible. Surprisingly, the relationship is fairly linear, with an R^2 of 0.994, at least up to the 1000 most common ingredients (due to running time, I did not extend all the way to 3958 ingredients). With 1000 of the most common ingredients, one can make roughly 3500 recipes. The curve does show a slight logistic tendency, as would be expected: adding ingredients rapidly expands the number of recipes you can make before petering out.\\n\\n# Machine Learning and Classification\\n\\nIn order to add a predictive component to this project, I utilized machine learning to predict the cuisine or dish of a recipe given its ingredients. Since we have established that cuisines differ substantially in terms of their ingredient usages, I expected fairly good results for these Cuisine and Dish classifiers.\\n\\nFor each recipe, I converted the ingredients of each recipe from a list of strings back to a single string so that I could utilize the bag-of-words model. This is admittedly a very inelegant solution, as there should exist a method to use the ingredients as the tokens for a model rather than deconvert back to a bag-of-words. Then I train-test-split with sklearn and fit a Naive Bayes and Logistic Regression model. I compared the predicted cuisines or dishes with the real cuisines and dishes and achieved surprisingly accurate results out-of-the-box:\\n\\nNull accuracy of cuisine prediction is \"Asian\", 555/8037 = 0.06906:\\n\\n- Multinomial Naive Bayes: 0.53383 accuracy. 7x better than null accuracy\\n- Logistic Regression: 0.43881 accuracy, 6x better tha null accuracy\\n\\nNull accuracy of dish prediction is \"Cake\", 633/16402 = 0.038592:\\n\\n- Multinomial Naive Bayes: 0.71080, 18x better than null accuracy\\n- Logistic Regression: 0.70299, 18x better than null accuracy\\n\\nA major reason why cuisine prediction was less accurate than dish prediction was that the cuisine data was not encoded as cleanly. Every recipe in the dishes_data dataset was assigned an explicit \"dish\" value, whereas every recipe in the cuisines_data dataset was NOT assigned an explicit cuisine. Many of the recipes in the cuisines_data dataset had multiple listings under cuisine. For example, a recipe could be listed as \"[Asian, Chinese]\". When previously counting the number of recipes that were Asian, I boolean-filtered for all recipes that had \"Asian\" within the string of \"cuisine\". So this recipe would have been counted twice, once for \"Asian\" and once for \"Chinese\"-- hence the total number of recipes does not add up.\\n\\nWhen doing this classification for cuisines, there is no easy way to disaggregate Asian and Chinese, as the computer must decide which of the two to classify any one recipe. So I kept all multi-cuisine designations rather than try to impute one over another. Rather than 25 cuisines to predict, I had 206, many combos of which only existed once in the dataset (e.g. \"[Barbecue, Mediterranean, Greek]\" or \"[Italian, Japanese]\"). Given this messiness, I was very pleased that the overall accuracy was still 7x better than null.\\n\\n# Next Steps\\n\\nMy next steps include:\\n\\n1. Refining cuisine and dish prediction. I only did minimal experimentation with model hyperparameters. I want to revisit the multiple-cuisine designation problem and ingredient-tokenization problem I mentioned above. I believe I can improve accuracy by another 15% for cuisines prediction, and perhaps another 10% for dish prediction.\\n\\n2. Publishing interesting results or infographics online, using static images or Bokeh/ D3\\n\\n3. Clustering analysis\\n\\n3. Pricing analysis if I can find an API that accepts ingredients and returns price per serving.\\n\\n\\n\\n\\n', metadata={'source': 'project_synopsis.txt'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the content of the first page of the document\n",
        "document[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "s3w8KQU0MzkL",
        "outputId": "31cb2918-2915-45c9-8a69-a1f31763a2d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Why this project?\\n\\nIronically, I enjoy cooking but I hate using recipes. In my opinion, recipes tend to encourage a slavish devotion to the recipe and divert attention from the more important part of cooking, the physical abilities (or, in my case, the lack thereof) of the chef. Foodies tend to privilege the provenance of obscure ingredients (\"coulis of feather saffron hand-picked from a seaside village in Morocco\"); I prefer the mundane but practical parts of cooking that get ignored in recipes (like freezing leftover sauce in ice cube trays, or the proper way to peel a mango).\\n\\nI am curious how much variation exists between dishes, and whether such variation is warranted. Are there really 5000 ways to cook a steak, or are many of these variations superfluous? Some chefs like Heston Blumenthal have taken an experimental approach to answering these questions, systematically and scientifically investigating every property of a dish, its ingredients, and its cooking methods to determine the \"best\" way to cook a dish. Since I do not have access to recipe instructions, I can only examine these recipes based on its ingredients. Nevertheless, I hope to use a data-science approach to see if the cooking wisdom of the crowds have arrived at the same answers, and if they match those of traditional experts.\\n\\n# Questions\\n\\nDish explorer:\\n\\n1. What are the most common ingredients for a particular dish?\\n2. What is the most unique recipe for each dish based on its ingredients?\\n3. Given a recipe, which recipes are most similar to it? (Recipe comparison)\\n4. Given a set of ingredients, which recipes can I make? (\"What\\'s-in-the-fridge\" prediction)\\n  - Which recipes can I make with a few more ingredients?\\n6. Given a set of ingredients, which dish and recipe is it most like? (Dish/recipe classifier)\\n\\nCuisine explorer:\\n\\n1. What are the most common ingredients for a particular cuisine?\\n2. What is the most unique recipe for each cuisine based on its ingredients?\\n3. Given a recipe, which recipes are most similar to it? (Recipe comparison)\\n4. Given a set of ingredients, which recipes can I make? (\"What\\'s-in-the-fridge\" prediction)\\n  - Which recipes can I make with a few more ingredients?\\n6. Given a set of ingredients, which dish and recipe is it most like? (Dish/recipe classifier)\\n\\nRatings and general exploration:\\n\\n1. Is there a relationship between recipe rating and number of ingredients?\\n2. Is there a relationship between recipe rating and recipe cost?\\n4. What other interesting relationships exist in the data?\\n\\n# Data Collection\\n\\nIn my preliminary project discussion, I had chosen to use the Spoonacular API, whose team had granted me an academic usage key. Spoonacular\\'s API was generally very satisfactory but I had concerns with the number and quality of recipes on their website, and the quality of the ingredients provided for each recipe. A week after the preliminary project presentation, I received academic usage approval for Yummly\\'s recipe API. While Spoonacular\\'s API had more bells and whistles and advanced features, I believed that Yummly\\'s database of recipes was generally of higher quality than Spoonacular\\'s. It also had access to over 1 million recipes compared to Spoonacular\\'s, and had better-formatted ingredients for each recipe. For these advantages, I switched data collection from Spoonacular to Yummly\\'s API.\\n\\nYummly\\'s API is accessed by querying an API endpoint and returns JSON-encoded recipe data. It was easy to set up and use. Since I divided the project into two parallel paths (cuisine and dish), the search queries were straightforward: either search by dish name (e.g. \"burger\") or specify a search by cuisine (e.g. cuisine-Chinese). JSON data was translated to Pandas dataframe structures using the Requests package and Pandas.\\n\\nI encountered difficulties with the API when I tried to retrieve too many recipes at once. Initially, I hoped to create a list of all cuisines or dishes and loop through those lists, requesting recipes during each loop. Unfortunately, the loop would break after a few cuisines or dishes due to problems on Yummy\\'s end. I opted to write separate CSV files for each dish and cuisine and concatenate them together into a master cuisines.csv and a master dishes.csv at the end rather than try to do it all in one go.\\n\\nIn total I made 454 calls to Yummly\\'s API over four days of data collection (accounting for missteps and experimentation). I wrote cuisine data into 25 separate CSVs before assembling them into cuisines_data.csv, and wrote dishes data into 45 separate CSVs before assembling into dishes_data.csv.\\n\\nEach row of data contained:\\n\\n- Yummly Recipe ID (string)\\n- Recipe Name (string)\\n- Yummly Rating (integer, 0-5)\\n- Cooking Time in Seconds (integer)\\n- Course (string)\\n- Cuisine (string)\\n- Ingredients (string)\\n\\n# Data Pre-Processing\\n\\nI conducted several processing steps on the two master data CSVs (dishes and cuisines):\\n\\n1. Ingredient parsing:\\n\\n\\tEach recipe\\'s ingredients were encoded as a single, unseparated string, including brackets: \"[ingredient 1, ingredient 2, ingredient 3]\". Since I wanted to examine each ingredient separately, I dropped the brackets from each string by reading from [1:-1]. Next, I separated by comma, then returned a list of these ingredient strings.\\n\\n\\tYummly\\'s ingredient specifications are imperfect. A human would be able to recognize that \"fresh pasta\", \"pasta\", and \"Giorgino pasta\" are essentially the same ingredient, but this code necessarily treats these as three separate and unique ingredients.\\n\\n2. Ingredient counts:\\n\\n\\tAfter parsing the ingredients from a single string to a list of strings, it was easy to calculate the number of ingredients in each recipe by creating a new column named \"ingredCount\" and setting each value to len(ingredients).\\n\\n3. Time conversion:\\n\\n\\tAs we are more accustomed to thinking about cooking times in minutes rather than seconds, I converted each recipe\\'s cooking time from seconds to minutes by dividing by 60 and populated a column named \"timeMins\".\\n\\n4. Munging:\\n\\n\\tI conducted an initial round of munging to ensure my data was sufficiently clean for analysis. \\n\\n\\tThe most important one was to drop any rows of cuisines data that had an empty \"cuisine\" value. Yummly\\'s \"Search by Cuisine\" API call returns recipes that have \"Chinese\" in the recipe in some shape or form-- even in the ingredients! So a sandwich recipe that employs \"French bread\" or an \"English muffin\" might have a cuisine value of \"French\" or \"English\". To avoid these ambiguities, I dropped any empty recipes without an explicit cuisine. This greatly reduced the number of recipes for some cuisines like English. I only did this for my cuisines_data.csv; since dishes_data.csv is unconcerned with dish origin, there was no need to drop dishes with empty cuisine values.\\n\\n\\tFor dishes_data.csv, I explicitly assigned each recipe to the search query that it was gathered from, using a new column named \"dish\". For example, all recipes from the burger.csv file were appended with \"dish=burger\". This dramatically simplified later analyses at the expense of some accuracy. I could not do this with cuisines_data, because a lot of the belonged to multiple cuisines and I wanted to preserve this complexity.\\n\\n\\tFor both dishes_data and cuisines_data, I filled in empty course values with \"Unknown\" rather than dropping these recipes.\\n\\n\\tFinally, I dropped duplicate recipes from both datasets. For example, a recipe for pad thai that is listed as both \"Asian\" and \"Thai\" within its cuisine value will show up twice in the dataset: first during the search for Asian dishes, then again during the search for Thai dishes.\\n\\n# Data Analysis:\\n\\n## Dish Dataset:\\n\\nDish dataset basic stats:\\n\\n- 16402 recipes\\n- 44 dishes (e.g. burger, burrito, steak, gumbo)\\n  - Dishes with the most recipes: cake (633), chowder (545), pancakes (457), roast chicken (444)\\n  - Dishes with the least recipes: donut (202), tacos (221), chili (265), turkey (275)\\n- 25 Cuisines: American is the most well-represented, with 396 recipes. Portuguese the least, with just 2 recipes\\n- Ratings: overwhelmingly 4-star (10984 of the 16402), some 3-star (3646) and 5-star (1683), very few 0, 1, or 2 stars (89 combined recipes)\\n- Cooking Time: mean of 65 minutes, max of 5760 minutes (4 days), min of 1 minute, median of 45 minutes\\n- Ingredients per recipe: mean of 9.9, max of 59 (swordfish ceviche), min of 1, median of 9\\n- 5385 unique ingredients:\\n  - Most common ingredients: Salt (6978 occurences, 0.43 frequency), butter (0.21), eggs (0.17), sugar (0.16), onions (0.15)\\n  - Least common ingredients: 1675 unique ingredients are only used in one recipe throughout the dataset. Examples: buckwheat noodles, gluten-free pie crust, canned snails, sugar-free Jell-O gelatin\\n\\n## Cuisine Dataset:\\n\\nCuisine dataset basic stats:\\n\\n- 8037 recipes\\n- 25 cuisines: Asian is the most well-represented, with 1414 recipes. English the least, with just 32 recipes.\\n- Ratings: overwhelmingly 4-star (4988 of the 8037), some 3-star (1517) and 5-star (1379), very few 0, 1, or 2 stars (153 combined recipes)\\n- Cooking Time: mean of 65 minutes, max of 1970 minutes (33 hours), min of 1 minute, median of 40 minutes\\n- Ingredients per recipe: mean of 10.1, max of 35 (Thai chicken tacos), min of 1, median of 9\\n- 3958 unique ingredients:\\n  - Most common ingredients: Salt (3325 occurences, 0.41 frequency), garlic (0.22), onions (0.20), olive oil (0.18)\\n  - Least common ingredients: 1401 unique ingredients are only used in one recipe throughout the dataset. Examples: chocolate candy, melon seeds, canned tuna, vegan yogurt, tarragon vinegar, unsalted almonds\\n\\n## Unique Ingredient Analysis:\\n\\nI wanted to examine the frequency and type of unique ingredients employed by each cuisine or each dish. I created a function with an input of a DataFrame that iterates through each recipe in the DataFrame and adds the contents of each recipe\\'s ingredients list to a summation list. This list is converted to a Pandas Series so that I can take advantage of Pandas\\' value_counts() method to count each appearance of an ingredient. The output of the value_counts() is saved as a column named \"instances\" representing the number of recipes that the ingredient appears in. I then calculated ingredient frequency by dividing the instances by the number of recipes in the dataset, and appended as a new column \"frequency\".\\n\\nSince this function takes a DataFrame as an input, it can be called on any dish, cuisine, or subset of dishes or cuisines to examine their unique ingredient counts and frequencies.\\n\\nFrom just cursory examination, it is easy to see how cuisines cook with different ingredients, and see which cuisines are similar to each other. For example, here are the most popular ingredients for some cuisines:\\n\\n- American: salt, butter, all-purpose flour, sugar, olive oil, water, onions, pepepr, unsalted butter, coarse salt\\n- French: salt, unsalted butter, sugar, butter, all-purpose flour, water, eggs, heavy cream, milk\\n- Chinese: soy sauce, corn starch, salt, sesame oil, garlic, sugar, water, scallions, ginger, oil\\n- Indian: salt, onions, garam masala, cumin seed, ground turmeric, garlic, ground cumin, oil, ginger, water\\n- Italian: salt, olive oil, parmesan cheese, garlic, extra-virgin olive oil, onions, garlic, pepper, eggs\\n- Thai: fish sauce, coconut milk, garlic, lime, soy sauce, salt, lime juice, vegetable oil, brown sugar\\n\\nIt is easy to see that American and French cuisine have many similarities, with frequent use of butter, sugar, flour, and salt. Chinese, Thai, and Indian cooking uses completely separate palettes of ingredients. This answers one of my questions, what are the most indicative ingredients for each cuisine?\\n\\n## Recipe Uniqueness Scoring:\\n\\nNext, I identified which recipes within a given dataset are the most \\'unique\\' in terms of their ingredients. For example, one expects that most chili recipes will contain tomato sauce, beans, ground beef, and onions. If a chili recipe does not use any of these ingredients and instead uses fruits or obscure meats, it is very different from the norm and should receive a high uniqueness score.\\n\\nThe first method of scoring is to take the mean of each recipe\\'s ingredient frequencies (e.g. (0.4+0.2+0.1) / 3 ). For each recipe\\'s ingredients, I sum the ingredients\\' unique ingredient frequency score calculated above, then divide by the number of ingredients in the recipe so as to not bias the scoring towards recipes with dozens of ingredients. I assign this score to a column named \"uniq_score1\".\\n\\nThe second method of scoring is to take the product of each recipe\\'s ingredient frequencies (e.g. 0.4 * 0.2 * 0.1). I assign this score to a column named \"uniq_score2\".\\n\\nBoth methods have their pros and cons and some more thinking is required to assess which better fits our intuition. Both methods produce broadly similar results, but the mean score seems to match our intuition slightly better than the product. The product tends to \\'reward\\' recipes with extremely rare ingredients whereas the mean method \\'rewards\\' these recipes to far lower extent.\\n\\nNevertheless, by examining the results I am more or less satisfied with these scoring methods. The most \\'typical\\' American Main Course, for example, is Southern Fried Chicken, which uses salt, butter, chicken, and oil. An extremely \\'atypical\\' dish is the swordfish ceviche, which uses 59 ingredients that are each very rare.\\n\\n## Data Relationships:\\n\\nI was disappointed by the inability to discover any meaningful relationships between some key metrics. For example, I hypothesized that recipes with extremely long (arduous) or extremely short (too simple) cooking times would receive lower ratings than recipes with a reasonable cooking time. Likewise recipes with many ingredients or very few ingredients. However, the ratings provided by Yummly were extremely uneven and extremely unlikely, with more than half the recipes in both cuisines_data and dishes_data holding a rating of 4. Almost no 0, 1, or 2-star recipes exist in either dataset, suggesting that Yummly is cooking the rating data somehow like Fandango\\'s movie ratings. \\n\\nAfter plotting cooking time vs. ratings and ingredient counts vs. ratings, it was clear that there are no correlations because of the incompleteness of the underlying ratings. \\n\\nThere may or may not be a relationship between ingredient counts and cooking times. According to the scatterplot it does not appear linear but rather bell-shaped. I did not yet fit a multidimensional regression curve to it yet.\\n\\n# Data Dictionary:\\n\\nMy final data dictionary:\\n\\n- Recipe ID (\\'id\\', string): set as index of both dishes_data.csv and cuisines_data.csv\\n- Recipe Name (\\'recipeName\\', string)\\n- Cooking Time in Seconds (\\'totalTimeInSeconds\\', integer)\\n- Cooking Time in Minutes (\\'timeMins\\', integer): calculated during processing stage\\n- Yummly Rating (\\'rating\\', integer): ratings from 0 to 5\\n- Course (\\'course\\', string): \"Unknown\" filled in for null values during processing stage\\n- Cuisine (\\'cuisine\\', string)\\n- Dish (\\'dish\\', string): for dishes_data.csv ONLY\\n- Ingredients (\\'ingredients\\', list of strings): parsed during processing stage\\n- Ingredient Count (\\'ingred_count\\', integer): calculated during processing stage\\n- Uniqueness Score #1 (\\'uniq_score1\\', floating): calculated during analysis stage; mean of ingredient frequencies\\n- Uniqueness Score #2 (\\'uniq_score2\\', floating): calculated during analysis stage; product of ingredient frequencies\\n- Can I Make This Dish? (\\'possible\\', boolean): determined during \"What Can I Make?\" analysis\\n\\nUnique ingredient data dictionary:\\n\\n- Ingredient name (\\'ingredient\\', string): set as index of uniq_ingred DataFrame\\n- Count (\\'instances\\', integer): number of recipes that the ingredient appears in\\n- Frequency (\\'frequency\\', floating): percentage of recipes that the ingredient appears in; calculated by dividing count by the number of recipes in the dataset\\n\\n# What Can I Make?\\n\\nIn order to find which recipes are possible given a set of supplies in a pantry, I employed Python\\'s .issubset() method, which determines if all of the objects in one set (the recipe) exist in another set (your pantry). I created a function which iterates through a dataset\\'s recipes and sets a boolean named \"possible\" to True if all ingredients in the recipe exist in your pantry. Again, the ingredients were parsed naively; the code has no way of knowing that \"fresh basil leaves\" can satisfy a requirement for \"basil leaves\". The computer treats these as two distinct ingredients.\\n\\nRather than type out a long list of items in a pantry, I decided the logical way for someone to stock a pantry would be to purchase the most common unique ingredients for whichever cuisine or dish the person was interested in making. For both cuisines_data and dishes_data, I stocked the pantry with the 100 most common unique ingredients in the dataset, then ran my function to see how many dishes were possible.\\n\\nI was suprised to see how few dishes were possible even with a well-stocked pantry. With the 100 most common unique ingredients, one can only make 114 out of the 8037 recipes in the cuisines_data dataset. To explore how adding additional ingredients to your pantry increased the number of recipes you can make, I successively added to the pantry and plotted how many recipes were possible. Surprisingly, the relationship is fairly linear, with an R^2 of 0.994, at least up to the 1000 most common ingredients (due to running time, I did not extend all the way to 3958 ingredients). With 1000 of the most common ingredients, one can make roughly 3500 recipes. The curve does show a slight logistic tendency, as would be expected: adding ingredients rapidly expands the number of recipes you can make before petering out.\\n\\n# Machine Learning and Classification\\n\\nIn order to add a predictive component to this project, I utilized machine learning to predict the cuisine or dish of a recipe given its ingredients. Since we have established that cuisines differ substantially in terms of their ingredient usages, I expected fairly good results for these Cuisine and Dish classifiers.\\n\\nFor each recipe, I converted the ingredients of each recipe from a list of strings back to a single string so that I could utilize the bag-of-words model. This is admittedly a very inelegant solution, as there should exist a method to use the ingredients as the tokens for a model rather than deconvert back to a bag-of-words. Then I train-test-split with sklearn and fit a Naive Bayes and Logistic Regression model. I compared the predicted cuisines or dishes with the real cuisines and dishes and achieved surprisingly accurate results out-of-the-box:\\n\\nNull accuracy of cuisine prediction is \"Asian\", 555/8037 = 0.06906:\\n\\n- Multinomial Naive Bayes: 0.53383 accuracy. 7x better than null accuracy\\n- Logistic Regression: 0.43881 accuracy, 6x better tha null accuracy\\n\\nNull accuracy of dish prediction is \"Cake\", 633/16402 = 0.038592:\\n\\n- Multinomial Naive Bayes: 0.71080, 18x better than null accuracy\\n- Logistic Regression: 0.70299, 18x better than null accuracy\\n\\nA major reason why cuisine prediction was less accurate than dish prediction was that the cuisine data was not encoded as cleanly. Every recipe in the dishes_data dataset was assigned an explicit \"dish\" value, whereas every recipe in the cuisines_data dataset was NOT assigned an explicit cuisine. Many of the recipes in the cuisines_data dataset had multiple listings under cuisine. For example, a recipe could be listed as \"[Asian, Chinese]\". When previously counting the number of recipes that were Asian, I boolean-filtered for all recipes that had \"Asian\" within the string of \"cuisine\". So this recipe would have been counted twice, once for \"Asian\" and once for \"Chinese\"-- hence the total number of recipes does not add up.\\n\\nWhen doing this classification for cuisines, there is no easy way to disaggregate Asian and Chinese, as the computer must decide which of the two to classify any one recipe. So I kept all multi-cuisine designations rather than try to impute one over another. Rather than 25 cuisines to predict, I had 206, many combos of which only existed once in the dataset (e.g. \"[Barbecue, Mediterranean, Greek]\" or \"[Italian, Japanese]\"). Given this messiness, I was very pleased that the overall accuracy was still 7x better than null.\\n\\n# Next Steps\\n\\nMy next steps include:\\n\\n1. Refining cuisine and dish prediction. I only did minimal experimentation with model hyperparameters. I want to revisit the multiple-cuisine designation problem and ingredient-tokenization problem I mentioned above. I believe I can improve accuracy by another 15% for cuisines prediction, and perhaps another 10% for dish prediction.\\n\\n2. Publishing interesting results or infographics online, using static images or Bokeh/ D3\\n\\n3. Clustering analysis\\n\\n3. Pricing analysis if I can find an API that accepts ingredients and returns price per serving.\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CharacterTextSplitter to split the document into chunks\n",
        "from langchain.text_splitter import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "DheBnL6zNEft"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CharacterTextSplitter object with specified chunk size and overlap\n",
        "splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
        "\n",
        "# Split the document into smaller text chunks\n",
        "text_chunks = splitter.split_documents(document)\n",
        "print(text_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9sZRL4uONo5",
        "outputId": "188faa32-22ae-4336-8cb3-7a0d08f29544"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 566, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 730, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 806, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 512, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 675, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1050, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 816, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 715, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 628, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 684, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 975, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 584, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 819, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 553, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 731, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 525, which is longer than the specified 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='# Why this project?', metadata={'source': 'project_synopsis.txt'}), Document(page_content='Ironically, I enjoy cooking but I hate using recipes. In my opinion, recipes tend to encourage a slavish devotion to the recipe and divert attention from the more important part of cooking, the physical abilities (or, in my case, the lack thereof) of the chef. Foodies tend to privilege the provenance of obscure ingredients (\"coulis of feather saffron hand-picked from a seaside village in Morocco\"); I prefer the mundane but practical parts of cooking that get ignored in recipes (like freezing leftover sauce in ice cube trays, or the proper way to peel a mango).', metadata={'source': 'project_synopsis.txt'}), Document(page_content='I am curious how much variation exists between dishes, and whether such variation is warranted. Are there really 5000 ways to cook a steak, or are many of these variations superfluous? Some chefs like Heston Blumenthal have taken an experimental approach to answering these questions, systematically and scientifically investigating every property of a dish, its ingredients, and its cooking methods to determine the \"best\" way to cook a dish. Since I do not have access to recipe instructions, I can only examine these recipes based on its ingredients. Nevertheless, I hope to use a data-science approach to see if the cooking wisdom of the crowds have arrived at the same answers, and if they match those of traditional experts.', metadata={'source': 'project_synopsis.txt'}), Document(page_content='# Questions\\n\\nDish explorer:\\n\\n1. What are the most common ingredients for a particular dish?\\n2. What is the most unique recipe for each dish based on its ingredients?\\n3. Given a recipe, which recipes are most similar to it? (Recipe comparison)\\n4. Given a set of ingredients, which recipes can I make? (\"What\\'s-in-the-fridge\" prediction)\\n  - Which recipes can I make with a few more ingredients?\\n6. Given a set of ingredients, which dish and recipe is it most like? (Dish/recipe classifier)', metadata={'source': 'project_synopsis.txt'}), Document(page_content='Cuisine explorer:\\n\\n1. What are the most common ingredients for a particular cuisine?\\n2. What is the most unique recipe for each cuisine based on its ingredients?\\n3. Given a recipe, which recipes are most similar to it? (Recipe comparison)\\n4. Given a set of ingredients, which recipes can I make? (\"What\\'s-in-the-fridge\" prediction)\\n  - Which recipes can I make with a few more ingredients?\\n6. Given a set of ingredients, which dish and recipe is it most like? (Dish/recipe classifier)', metadata={'source': 'project_synopsis.txt'}), Document(page_content='Ratings and general exploration:\\n\\n1. Is there a relationship between recipe rating and number of ingredients?\\n2. Is there a relationship between recipe rating and recipe cost?\\n4. What other interesting relationships exist in the data?\\n\\n# Data Collection', metadata={'source': 'project_synopsis.txt'}), Document(page_content=\"In my preliminary project discussion, I had chosen to use the Spoonacular API, whose team had granted me an academic usage key. Spoonacular's API was generally very satisfactory but I had concerns with the number and quality of recipes on their website, and the quality of the ingredients provided for each recipe. A week after the preliminary project presentation, I received academic usage approval for Yummly's recipe API. While Spoonacular's API had more bells and whistles and advanced features, I believed that Yummly's database of recipes was generally of higher quality than Spoonacular's. It also had access to over 1 million recipes compared to Spoonacular's, and had better-formatted ingredients for each recipe. For these advantages, I switched data collection from Spoonacular to Yummly's API.\", metadata={'source': 'project_synopsis.txt'}), Document(page_content='Yummly\\'s API is accessed by querying an API endpoint and returns JSON-encoded recipe data. It was easy to set up and use. Since I divided the project into two parallel paths (cuisine and dish), the search queries were straightforward: either search by dish name (e.g. \"burger\") or specify a search by cuisine (e.g. cuisine-Chinese). JSON data was translated to Pandas dataframe structures using the Requests package and Pandas.', metadata={'source': 'project_synopsis.txt'}), Document(page_content=\"I encountered difficulties with the API when I tried to retrieve too many recipes at once. Initially, I hoped to create a list of all cuisines or dishes and loop through those lists, requesting recipes during each loop. Unfortunately, the loop would break after a few cuisines or dishes due to problems on Yummy's end. I opted to write separate CSV files for each dish and cuisine and concatenate them together into a master cuisines.csv and a master dishes.csv at the end rather than try to do it all in one go.\", metadata={'source': 'project_synopsis.txt'}), Document(page_content=\"In total I made 454 calls to Yummly's API over four days of data collection (accounting for missteps and experimentation). I wrote cuisine data into 25 separate CSVs before assembling them into cuisines_data.csv, and wrote dishes data into 45 separate CSVs before assembling into dishes_data.csv.\\n\\nEach row of data contained:\", metadata={'source': 'project_synopsis.txt'}), Document(page_content='Each row of data contained:\\n\\n- Yummly Recipe ID (string)\\n- Recipe Name (string)\\n- Yummly Rating (integer, 0-5)\\n- Cooking Time in Seconds (integer)\\n- Course (string)\\n- Cuisine (string)\\n- Ingredients (string)\\n\\n# Data Pre-Processing\\n\\nI conducted several processing steps on the two master data CSVs (dishes and cuisines):\\n\\n1. Ingredient parsing:', metadata={'source': 'project_synopsis.txt'}), Document(page_content='1. Ingredient parsing:\\n\\n\\tEach recipe\\'s ingredients were encoded as a single, unseparated string, including brackets: \"[ingredient 1, ingredient 2, ingredient 3]\". Since I wanted to examine each ingredient separately, I dropped the brackets from each string by reading from [1:-1]. Next, I separated by comma, then returned a list of these ingredient strings.', metadata={'source': 'project_synopsis.txt'}), Document(page_content='Yummly\\'s ingredient specifications are imperfect. A human would be able to recognize that \"fresh pasta\", \"pasta\", and \"Giorgino pasta\" are essentially the same ingredient, but this code necessarily treats these as three separate and unique ingredients.\\n\\n2. Ingredient counts:', metadata={'source': 'project_synopsis.txt'}), Document(page_content='2. Ingredient counts:\\n\\n\\tAfter parsing the ingredients from a single string to a list of strings, it was easy to calculate the number of ingredients in each recipe by creating a new column named \"ingredCount\" and setting each value to len(ingredients).\\n\\n3. Time conversion:\\n\\n\\tAs we are more accustomed to thinking about cooking times in minutes rather than seconds, I converted each recipe\\'s cooking time from seconds to minutes by dividing by 60 and populated a column named \"timeMins\".\\n\\n4. Munging:', metadata={'source': 'project_synopsis.txt'}), Document(page_content='4. Munging:\\n\\n\\tI conducted an initial round of munging to ensure my data was sufficiently clean for analysis.', metadata={'source': 'project_synopsis.txt'}), Document(page_content='The most important one was to drop any rows of cuisines data that had an empty \"cuisine\" value. Yummly\\'s \"Search by Cuisine\" API call returns recipes that have \"Chinese\" in the recipe in some shape or form-- even in the ingredients! So a sandwich recipe that employs \"French bread\" or an \"English muffin\" might have a cuisine value of \"French\" or \"English\". To avoid these ambiguities, I dropped any empty recipes without an explicit cuisine. This greatly reduced the number of recipes for some cuisines like English. I only did this for my cuisines_data.csv; since dishes_data.csv is unconcerned with dish origin, there was no need to drop dishes with empty cuisine values.', metadata={'source': 'project_synopsis.txt'}), Document(page_content='For dishes_data.csv, I explicitly assigned each recipe to the search query that it was gathered from, using a new column named \"dish\". For example, all recipes from the burger.csv file were appended with \"dish=burger\". This dramatically simplified later analyses at the expense of some accuracy. I could not do this with cuisines_data, because a lot of the belonged to multiple cuisines and I wanted to preserve this complexity.', metadata={'source': 'project_synopsis.txt'}), Document(page_content='For both dishes_data and cuisines_data, I filled in empty course values with \"Unknown\" rather than dropping these recipes.\\n\\n\\tFinally, I dropped duplicate recipes from both datasets. For example, a recipe for pad thai that is listed as both \"Asian\" and \"Thai\" within its cuisine value will show up twice in the dataset: first during the search for Asian dishes, then again during the search for Thai dishes.\\n\\n# Data Analysis:\\n\\n## Dish Dataset:\\n\\nDish dataset basic stats:', metadata={'source': 'project_synopsis.txt'}), Document(page_content='- 16402 recipes\\n- 44 dishes (e.g. burger, burrito, steak, gumbo)\\n  - Dishes with the most recipes: cake (633), chowder (545), pancakes (457), roast chicken (444)\\n  - Dishes with the least recipes: donut (202), tacos (221), chili (265), turkey (275)\\n- 25 Cuisines: American is the most well-represented, with 396 recipes. Portuguese the least, with just 2 recipes\\n- Ratings: overwhelmingly 4-star (10984 of the 16402), some 3-star (3646) and 5-star (1683), very few 0, 1, or 2 stars (89 combined recipes)\\n- Cooking Time: mean of 65 minutes, max of 5760 minutes (4 days), min of 1 minute, median of 45 minutes\\n- Ingredients per recipe: mean of 9.9, max of 59 (swordfish ceviche), min of 1, median of 9\\n- 5385 unique ingredients:\\n  - Most common ingredients: Salt (6978 occurences, 0.43 frequency), butter (0.21), eggs (0.17), sugar (0.16), onions (0.15)\\n  - Least common ingredients: 1675 unique ingredients are only used in one recipe throughout the dataset. Examples: buckwheat noodles, gluten-free pie crust, canned snails, sugar-free Jell-O gelatin', metadata={'source': 'project_synopsis.txt'}), Document(page_content='## Cuisine Dataset:\\n\\nCuisine dataset basic stats:', metadata={'source': 'project_synopsis.txt'}), Document(page_content='- 8037 recipes\\n- 25 cuisines: Asian is the most well-represented, with 1414 recipes. English the least, with just 32 recipes.\\n- Ratings: overwhelmingly 4-star (4988 of the 8037), some 3-star (1517) and 5-star (1379), very few 0, 1, or 2 stars (153 combined recipes)\\n- Cooking Time: mean of 65 minutes, max of 1970 minutes (33 hours), min of 1 minute, median of 40 minutes\\n- Ingredients per recipe: mean of 10.1, max of 35 (Thai chicken tacos), min of 1, median of 9\\n- 3958 unique ingredients:\\n  - Most common ingredients: Salt (3325 occurences, 0.41 frequency), garlic (0.22), onions (0.20), olive oil (0.18)\\n  - Least common ingredients: 1401 unique ingredients are only used in one recipe throughout the dataset. Examples: chocolate candy, melon seeds, canned tuna, vegan yogurt, tarragon vinegar, unsalted almonds', metadata={'source': 'project_synopsis.txt'}), Document(page_content='## Unique Ingredient Analysis:', metadata={'source': 'project_synopsis.txt'}), Document(page_content='I wanted to examine the frequency and type of unique ingredients employed by each cuisine or each dish. I created a function with an input of a DataFrame that iterates through each recipe in the DataFrame and adds the contents of each recipe\\'s ingredients list to a summation list. This list is converted to a Pandas Series so that I can take advantage of Pandas\\' value_counts() method to count each appearance of an ingredient. The output of the value_counts() is saved as a column named \"instances\" representing the number of recipes that the ingredient appears in. I then calculated ingredient frequency by dividing the instances by the number of recipes in the dataset, and appended as a new column \"frequency\".', metadata={'source': 'project_synopsis.txt'}), Document(page_content='Since this function takes a DataFrame as an input, it can be called on any dish, cuisine, or subset of dishes or cuisines to examine their unique ingredient counts and frequencies.\\n\\nFrom just cursory examination, it is easy to see how cuisines cook with different ingredients, and see which cuisines are similar to each other. For example, here are the most popular ingredients for some cuisines:', metadata={'source': 'project_synopsis.txt'}), Document(page_content='- American: salt, butter, all-purpose flour, sugar, olive oil, water, onions, pepepr, unsalted butter, coarse salt\\n- French: salt, unsalted butter, sugar, butter, all-purpose flour, water, eggs, heavy cream, milk\\n- Chinese: soy sauce, corn starch, salt, sesame oil, garlic, sugar, water, scallions, ginger, oil\\n- Indian: salt, onions, garam masala, cumin seed, ground turmeric, garlic, ground cumin, oil, ginger, water\\n- Italian: salt, olive oil, parmesan cheese, garlic, extra-virgin olive oil, onions, garlic, pepper, eggs\\n- Thai: fish sauce, coconut milk, garlic, lime, soy sauce, salt, lime juice, vegetable oil, brown sugar', metadata={'source': 'project_synopsis.txt'}), Document(page_content='It is easy to see that American and French cuisine have many similarities, with frequent use of butter, sugar, flour, and salt. Chinese, Thai, and Indian cooking uses completely separate palettes of ingredients. This answers one of my questions, what are the most indicative ingredients for each cuisine?\\n\\n## Recipe Uniqueness Scoring:', metadata={'source': 'project_synopsis.txt'}), Document(page_content=\"## Recipe Uniqueness Scoring:\\n\\nNext, I identified which recipes within a given dataset are the most 'unique' in terms of their ingredients. For example, one expects that most chili recipes will contain tomato sauce, beans, ground beef, and onions. If a chili recipe does not use any of these ingredients and instead uses fruits or obscure meats, it is very different from the norm and should receive a high uniqueness score.\", metadata={'source': 'project_synopsis.txt'}), Document(page_content='The first method of scoring is to take the mean of each recipe\\'s ingredient frequencies (e.g. (0.4+0.2+0.1) / 3 ). For each recipe\\'s ingredients, I sum the ingredients\\' unique ingredient frequency score calculated above, then divide by the number of ingredients in the recipe so as to not bias the scoring towards recipes with dozens of ingredients. I assign this score to a column named \"uniq_score1\".', metadata={'source': 'project_synopsis.txt'}), Document(page_content='The second method of scoring is to take the product of each recipe\\'s ingredient frequencies (e.g. 0.4 * 0.2 * 0.1). I assign this score to a column named \"uniq_score2\".', metadata={'source': 'project_synopsis.txt'}), Document(page_content=\"Both methods have their pros and cons and some more thinking is required to assess which better fits our intuition. Both methods produce broadly similar results, but the mean score seems to match our intuition slightly better than the product. The product tends to 'reward' recipes with extremely rare ingredients whereas the mean method 'rewards' these recipes to far lower extent.\", metadata={'source': 'project_synopsis.txt'}), Document(page_content=\"Nevertheless, by examining the results I am more or less satisfied with these scoring methods. The most 'typical' American Main Course, for example, is Southern Fried Chicken, which uses salt, butter, chicken, and oil. An extremely 'atypical' dish is the swordfish ceviche, which uses 59 ingredients that are each very rare.\\n\\n## Data Relationships:\", metadata={'source': 'project_synopsis.txt'}), Document(page_content=\"I was disappointed by the inability to discover any meaningful relationships between some key metrics. For example, I hypothesized that recipes with extremely long (arduous) or extremely short (too simple) cooking times would receive lower ratings than recipes with a reasonable cooking time. Likewise recipes with many ingredients or very few ingredients. However, the ratings provided by Yummly were extremely uneven and extremely unlikely, with more than half the recipes in both cuisines_data and dishes_data holding a rating of 4. Almost no 0, 1, or 2-star recipes exist in either dataset, suggesting that Yummly is cooking the rating data somehow like Fandango's movie ratings.\", metadata={'source': 'project_synopsis.txt'}), Document(page_content='After plotting cooking time vs. ratings and ingredient counts vs. ratings, it was clear that there are no correlations because of the incompleteness of the underlying ratings. \\n\\nThere may or may not be a relationship between ingredient counts and cooking times. According to the scatterplot it does not appear linear but rather bell-shaped. I did not yet fit a multidimensional regression curve to it yet.\\n\\n# Data Dictionary:\\n\\nMy final data dictionary:', metadata={'source': 'project_synopsis.txt'}), Document(page_content='- Recipe ID (\\'id\\', string): set as index of both dishes_data.csv and cuisines_data.csv\\n- Recipe Name (\\'recipeName\\', string)\\n- Cooking Time in Seconds (\\'totalTimeInSeconds\\', integer)\\n- Cooking Time in Minutes (\\'timeMins\\', integer): calculated during processing stage\\n- Yummly Rating (\\'rating\\', integer): ratings from 0 to 5\\n- Course (\\'course\\', string): \"Unknown\" filled in for null values during processing stage\\n- Cuisine (\\'cuisine\\', string)\\n- Dish (\\'dish\\', string): for dishes_data.csv ONLY\\n- Ingredients (\\'ingredients\\', list of strings): parsed during processing stage\\n- Ingredient Count (\\'ingred_count\\', integer): calculated during processing stage\\n- Uniqueness Score #1 (\\'uniq_score1\\', floating): calculated during analysis stage; mean of ingredient frequencies\\n- Uniqueness Score #2 (\\'uniq_score2\\', floating): calculated during analysis stage; product of ingredient frequencies\\n- Can I Make This Dish? (\\'possible\\', boolean): determined during \"What Can I Make?\" analysis', metadata={'source': 'project_synopsis.txt'}), Document(page_content=\"Unique ingredient data dictionary:\\n\\n- Ingredient name ('ingredient', string): set as index of uniq_ingred DataFrame\\n- Count ('instances', integer): number of recipes that the ingredient appears in\\n- Frequency ('frequency', floating): percentage of recipes that the ingredient appears in; calculated by dividing count by the number of recipes in the dataset\\n\\n# What Can I Make?\", metadata={'source': 'project_synopsis.txt'}), Document(page_content='In order to find which recipes are possible given a set of supplies in a pantry, I employed Python\\'s .issubset() method, which determines if all of the objects in one set (the recipe) exist in another set (your pantry). I created a function which iterates through a dataset\\'s recipes and sets a boolean named \"possible\" to True if all ingredients in the recipe exist in your pantry. Again, the ingredients were parsed naively; the code has no way of knowing that \"fresh basil leaves\" can satisfy a requirement for \"basil leaves\". The computer treats these as two distinct ingredients.', metadata={'source': 'project_synopsis.txt'}), Document(page_content='Rather than type out a long list of items in a pantry, I decided the logical way for someone to stock a pantry would be to purchase the most common unique ingredients for whichever cuisine or dish the person was interested in making. For both cuisines_data and dishes_data, I stocked the pantry with the 100 most common unique ingredients in the dataset, then ran my function to see how many dishes were possible.', metadata={'source': 'project_synopsis.txt'}), Document(page_content='I was suprised to see how few dishes were possible even with a well-stocked pantry. With the 100 most common unique ingredients, one can only make 114 out of the 8037 recipes in the cuisines_data dataset. To explore how adding additional ingredients to your pantry increased the number of recipes you can make, I successively added to the pantry and plotted how many recipes were possible. Surprisingly, the relationship is fairly linear, with an R^2 of 0.994, at least up to the 1000 most common ingredients (due to running time, I did not extend all the way to 3958 ingredients). With 1000 of the most common ingredients, one can make roughly 3500 recipes. The curve does show a slight logistic tendency, as would be expected: adding ingredients rapidly expands the number of recipes you can make before petering out.', metadata={'source': 'project_synopsis.txt'}), Document(page_content='# Machine Learning and Classification\\n\\nIn order to add a predictive component to this project, I utilized machine learning to predict the cuisine or dish of a recipe given its ingredients. Since we have established that cuisines differ substantially in terms of their ingredient usages, I expected fairly good results for these Cuisine and Dish classifiers.', metadata={'source': 'project_synopsis.txt'}), Document(page_content='For each recipe, I converted the ingredients of each recipe from a list of strings back to a single string so that I could utilize the bag-of-words model. This is admittedly a very inelegant solution, as there should exist a method to use the ingredients as the tokens for a model rather than deconvert back to a bag-of-words. Then I train-test-split with sklearn and fit a Naive Bayes and Logistic Regression model. I compared the predicted cuisines or dishes with the real cuisines and dishes and achieved surprisingly accurate results out-of-the-box:', metadata={'source': 'project_synopsis.txt'}), Document(page_content='Null accuracy of cuisine prediction is \"Asian\", 555/8037 = 0.06906:\\n\\n- Multinomial Naive Bayes: 0.53383 accuracy. 7x better than null accuracy\\n- Logistic Regression: 0.43881 accuracy, 6x better tha null accuracy\\n\\nNull accuracy of dish prediction is \"Cake\", 633/16402 = 0.038592:\\n\\n- Multinomial Naive Bayes: 0.71080, 18x better than null accuracy\\n- Logistic Regression: 0.70299, 18x better than null accuracy', metadata={'source': 'project_synopsis.txt'}), Document(page_content='A major reason why cuisine prediction was less accurate than dish prediction was that the cuisine data was not encoded as cleanly. Every recipe in the dishes_data dataset was assigned an explicit \"dish\" value, whereas every recipe in the cuisines_data dataset was NOT assigned an explicit cuisine. Many of the recipes in the cuisines_data dataset had multiple listings under cuisine. For example, a recipe could be listed as \"[Asian, Chinese]\". When previously counting the number of recipes that were Asian, I boolean-filtered for all recipes that had \"Asian\" within the string of \"cuisine\". So this recipe would have been counted twice, once for \"Asian\" and once for \"Chinese\"-- hence the total number of recipes does not add up.', metadata={'source': 'project_synopsis.txt'}), Document(page_content='When doing this classification for cuisines, there is no easy way to disaggregate Asian and Chinese, as the computer must decide which of the two to classify any one recipe. So I kept all multi-cuisine designations rather than try to impute one over another. Rather than 25 cuisines to predict, I had 206, many combos of which only existed once in the dataset (e.g. \"[Barbecue, Mediterranean, Greek]\" or \"[Italian, Japanese]\"). Given this messiness, I was very pleased that the overall accuracy was still 7x better than null.', metadata={'source': 'project_synopsis.txt'}), Document(page_content='# Next Steps\\n\\nMy next steps include:\\n\\n1. Refining cuisine and dish prediction. I only did minimal experimentation with model hyperparameters. I want to revisit the multiple-cuisine designation problem and ingredient-tokenization problem I mentioned above. I believe I can improve accuracy by another 15% for cuisines prediction, and perhaps another 10% for dish prediction.\\n\\n2. Publishing interesting results or infographics online, using static images or Bokeh/ D3\\n\\n3. Clustering analysis', metadata={'source': 'project_synopsis.txt'}), Document(page_content='3. Clustering analysis\\n\\n3. Pricing analysis if I can find an API that accepts ingredients and returns price per serving.', metadata={'source': 'project_synopsis.txt'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first text chunk\n",
        "text_chunks[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kbpK4GdO_2b",
        "outputId": "f03d193a-3da2-4ae2-fc7e-f4aeb5be81ed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='# Why this project?', metadata={'source': 'project_synopsis.txt'})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the content of the first text chunk\n",
        "text_chunks[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6zsmqZoUPIvW",
        "outputId": "395b890a-9f6e-4362-9949-c453baa51c0f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Why this project?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the content of the third text chunk\n",
        "text_chunks[2].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "3ytuP87ePPEa",
        "outputId": "0618dd61-42d6-4e6f-a2e1-920414e144c9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am curious how much variation exists between dishes, and whether such variation is warranted. Are there really 5000 ways to cook a steak, or are many of these variations superfluous? Some chefs like Heston Blumenthal have taken an experimental approach to answering these questions, systematically and scientifically investigating every property of a dish, its ingredients, and its cooking methods to determine the \"best\" way to cook a dish. Since I do not have access to recipe instructions, I can only examine these recipes based on its ingredients. Nevertheless, I hope to use a data-science approach to see if the cooking wisdom of the crowds have arrived at the same answers, and if they match those of traditional experts.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import userdata from google.colab to fetch the OpenAI API key\n",
        "from google.colab import userdata\n",
        "\n",
        "# Fetch the OpenAI API key from user data\n",
        "OPENAI_API_KEY = userdata.get('OPEN_AI_KEY')"
      ],
      "metadata": {
        "id": "YVoCXrq2T6jF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules for creating embeddings and using Weaviate\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Weaviate\n",
        "from weaviate import Client\n",
        "from weaviate.embedded import EmbeddedOptions"
      ],
      "metadata": {
        "id": "qdooUYwtPRX2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a Weaviate client with embedded options\n",
        "client = Client(\n",
        "    embedded_options=EmbeddedOptions()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhCKL3gIRo9O",
        "outputId": "9c925c5e-443c-425d-abcd-306d347db4c9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started /root/.cache/weaviate-embedded: process ID 53728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vector store using Weaviate from the text chunks and OpenAI embeddings\n",
        "vector_store = Weaviate.from_documents(client = client,\n",
        "                                       documents = text_chunks,\n",
        "                                       embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY),\n",
        "                                       by_text=False)"
      ],
      "metadata": {
        "id": "K_ud3Y11SXjB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retriever from the vector store\n",
        "retriever = vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "EGxUNcsdUiFs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ChatPromptTemplate for creating the prompt template\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "3L72JVMrWqZg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the template for the chat prompt\n",
        "template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hygxsdX5WtvP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ChatPromptTemplate object from the defined template\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "bCdpL1QHW8XP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the chat prompt template\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1W7Fb-6XgZH",
        "outputId": "1f01c092-e38e-42ae-fdba-32b6d9d8a396"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse ten sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules for chat models and output parsing\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "GGATU9RoXiWB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a ChatOpenAI model with the specified API key and model name\n",
        "llm = ChatOpenAI(api_key=OPENAI_API_KEY,model_name=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "UoL3xr9damOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2d490d-d2c7-44ff-93e6-7e6d681d9dc4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an output parser for string output\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "4hZyAyK5eJg4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a query to ask about the project\n",
        "query = \"Tell me about this project\""
      ],
      "metadata": {
        "id": "oZ-rOc2za6o8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RAG (retrieval-augmented generation) chain\n",
        "rag_chain = (\n",
        "    {\"context\":retriever,\"question\":RunnablePassthrough()}|\n",
        "    prompt|\n",
        "    llm|\n",
        "    output_parser\n",
        ")"
      ],
      "metadata": {
        "id": "ZX0ef3gKbc2R"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the RAG chain with the query and print the result\n",
        "rag_chain.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "3XsBthmEeNQ4",
        "outputId": "be3e4582-08fc-4ec9-9696-3c97f3e6a367"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This project focuses on utilizing machine learning to predict the cuisine or dish of a recipe based on its ingredients. The project initially used the Spoonacular API but later switched to Yummly's recipe API for better quality and quantity of recipes. The next steps include refining cuisine and dish prediction through experimentation with model hyperparameters. The goal is to improve accuracy by 15% for cuisines prediction and 10% for dish prediction. The project also plans to publish interesting results or infographics online using static images or Bokeh/D3. Additionally, clustering analysis is on the agenda for this project.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}